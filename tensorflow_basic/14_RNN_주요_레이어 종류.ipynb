{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249f6671-8f6d-45da-84d7-0983dc4b198a",
   "metadata": {},
   "source": [
    "### * RNN 주요 레이어 종류\n",
    "#### (1) SimpleRNN :가장 간단한 형태의 RNN레이어, 활성화 함수로 tanh가 사용됨(tanh: -1 ~ 1 사이의 값을 반환)\n",
    "#### (2) LSTM(Long short Term Memory) : 입력 데이터와 출력 사이의 거리가 멀어질수로 연관 관계가 적어진다(Long Term Dependency,장기의존성 문제), LSTM은 장기 의존성 문제를 해결하기 위해 출력값외에 셀상태(cell state)값을 출력함, 활성화 함수로 tanh외에 sigmoid가 사용됨\n",
    "#### (3) GRU(Gated Recurent Unit) : 뉴욕대 조경현 교수 등이 제안, LSTM보다 구조가 간단하고 성능이 우수함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1854353d-9dae-4733-9479-76d0f4b8cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff4ef3c-a010-4197-ae4c-0722191f1425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4, 1) (6,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 4, 1), dtype=float32, numpy=\n",
       "array([[[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.]],\n",
       "\n",
       "       [[1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.]],\n",
       "\n",
       "       [[2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.]],\n",
       "\n",
       "       [[3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.]],\n",
       "\n",
       "       [[4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.]],\n",
       "\n",
       "       [[5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.]]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence data\n",
    "X = np.array([[0,1,2,3],\n",
    "              [1,2,3,4],\n",
    "              [2,3,4,5],\n",
    "              [3,4,5,6],\n",
    "              [4,5,6,7],\n",
    "              [5,6,7,8]],dtype=np.float32)\n",
    "\n",
    "x_data = tf.reshape(X,(-1,4,1))  # (6,4,1)\n",
    "\n",
    "y_data = np.array([4,5,6,7,8,9],dtype=np.float32)\n",
    "\n",
    "print(x_data.shape,y_data.shape)\n",
    "# print(type(x_data),type(y_data))\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfabda1b-aaba-4927-b73a-454e2e999094",
   "metadata": {},
   "source": [
    "### [1] SimpleRNN\n",
    "#### 가장 간단한 형태의 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a23adeb-31cb-415f-97b1-9e8938bb53e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 4, 300)            90600     \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 300)               180300    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,201\n",
      "Trainable params: 271,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# RNN 순환 신경만 구현 : SimpleRNN\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # X: (N,D) , Wx:(D,H) Wh: (H,H) b: H\n",
    "    #    (6,1) ,    (1,300)     (300,300)    300    --> 1*300+300*300+300 = 90600 param\n",
    "    # (N,T,D) : (6,4,1) --> (N,T,H) : (6,4,300) , T 는 sequence_lenth, H 는  Hidden Size\n",
    "    # return_sequences=True는 3차원(N,T,D)으로 출력\n",
    "    # return_sequences=False는 2차원(N,H)으로 출력, 기본값\n",
    "    tf.keras.layers.SimpleRNN(units=300, return_sequences=True, input_shape=(4,1)), #(?,4,1) ?는 batch_size가 얼만지 모르기 때문에 입력 X\n",
    "    tf.keras.layers.SimpleRNN(units=300),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "model.summary()                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6e5e91-d264-4466-84cd-3daf098b7bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 35.0873\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9879\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.9095\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.1622\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.3662\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.5376\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0641\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.9874\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.7487\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8289\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1060\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8980\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0272\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1766\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1465\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9361\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7047\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6444\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7782\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8836\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7842\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5913\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5020\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5441\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5999\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5669\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4524\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3568\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3654\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4084\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3456\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2313\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1965\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2218\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2066\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1372\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1020\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1358\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1329\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0763\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0688\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0933\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0731\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0386\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0530\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0600\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0286\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0304\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0453\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0284\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0226\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0394\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0294\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0231\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0354\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0289\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0221\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0310\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0250\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0203\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0265\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0216\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0189\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0237\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0188\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0180\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0205\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0160\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0161\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0169\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0132\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0141\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0138\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0114\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0127\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0117\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0106\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0116\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0102\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0100\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0103\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0090\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0093\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0090\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0083\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0086\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0079\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0077\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0077\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0070\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0071\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0067\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0064\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0063\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0058\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0058\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0056\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0053\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0053\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0050\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "[[4.016374 ]\n",
      " [4.9354215]\n",
      " [5.9639964]\n",
      " [7.0666337]\n",
      " [8.0651865]\n",
      " [8.876724 ]]\n"
     ]
    }
   ],
   "source": [
    "# 학습 및 예측\n",
    "model.fit(x_data,y_data,epochs=100)\n",
    "print(model.predict(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee40216-c163-4ad0-85b3-21feb02730c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "[[9.49235]]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "[[1.2670304]]\n"
     ]
    }
   ],
   "source": [
    "# 학습되지 않은 입력 데이터에 대한 예측 결과\n",
    "print(model.predict(np.array([[[6.],[7.],[8.],[9.]]])))\n",
    "print(model.predict(np.array([[[-1.],[0.],[1.],[2.]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39fcbb67-533e-4169-b003-fdf79fec4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0049368334002792835"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "model.evaluate(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3a87a7-1ccf-4173-8e31-6be33636e88c",
   "metadata": {},
   "source": [
    "### [2] LSTM(Long short Term Memory)\n",
    "#### 입력 데이터와 출력 사이의 거리가 멀어질수로 연관 관계가 적어진다(Long Term Dependency,장기의존성 문제)\n",
    "#### LSTM은 장기 의존성 문제를 해결하기 위해 출력값외에 셀상태(cell state)값을 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf089bb-75e7-4ca7-ae7c-552c3f653284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 4, 300)            362400    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 300)               721200    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,083,901\n",
      "Trainable params: 1,083,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# RNN 순환 신경망 구현 : LSTM\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # X: (N,D) , Wx:(D,4H)  Wh: (4H,H)    b: 4H\n",
    "    #    (6,1) ,    (1,300)     (300,300)    4*300    --> 1*4*300+4*300*300+4*300 = 4*90600 param\n",
    "    # (N,T,D) : (6,4,1) --> (N,T,H) : (6,4,300) , T 는 sequence_lenth, H 는  Hidden Size\n",
    "    \n",
    "    tf.keras.layers.LSTM(units=300, return_sequences=True, input_shape=[4,1]), #(?,4,1) ?는 batch_size가 얼만지 모르기 때문에 입력 X\n",
    "    tf.keras.layers.LSTM(units=300),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "model.summary()                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d475d74-3ffc-475d-b062-ca764a4077b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 305ms/step\n",
      "[[3.3811972]\n",
      " [5.109454 ]\n",
      " [6.3834367]\n",
      " [7.3105726]\n",
      " [7.99787  ]\n",
      " [8.518563 ]]\n"
     ]
    }
   ],
   "source": [
    "# 학습 및 예측\n",
    "model.fit(x_data,y_data,epochs=100,verbose= 0)\n",
    "print(model.predict(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2373e15-4bd3-48f1-8532-a3d045ea9cd2",
   "metadata": {},
   "source": [
    "### [3] GRU(Gated Recurent Unit)\n",
    "#### 뉴욕대 조경현 교수 등이 제안, LSTM보다 구조가 간단하고 성능이 우수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07638147-5785-4dbb-a9a5-c37feb5e31a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 4, 300)            272700    \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 300)               541800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814,801\n",
      "Trainable params: 814,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# RNN 순환 신경망 구현  : GRU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(units=300, return_sequences=True, input_shape=[4,1]),\n",
    "    tf.keras.layers.GRU(units=300),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740afc9-8e4e-412c-b17c-4ac5386f7db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
